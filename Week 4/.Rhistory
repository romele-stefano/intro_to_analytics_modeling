load("C:/Users/ROMEST/Downloads/.RData")
accuracy
#Create 10 equally size folds
data_div <- cut(seq(1,nrow(data)),breaks = 10,labels = T)
nrow(data)
#Create 10 equally size folds
data_div <- cut(seq(1,nrow(data)),breaks = 2,labels = T)
#Create 10 equally size folds
data_div <- cut(seq(1,nrow(data)),breaks = 10,labels = T)
#Create 10 equally size folds
data_div <- cut(seq(1,nrow(data)),breaks = 10,labels = F)
data_div
set.seed(888)
#Randomly shuffle the data
data_shuffle <- data[sample(nrow(data)),]
head(data_shuffle)
head(data)
str(data_Shuffle)
str(data_shuffle)
# k-fold cross validation
library("caret")
install.packages("caret")
library("carte")
library("cartet")
library("caret")
?trainControl
# folds
folds <- trainControl(method = "cv", number = 10)
folds
round(nrow(data) / groups, 0)
round(nrow(data) / folds, 0)
round(nrow(data) / folds)
nrow(data)
type(nrow(data))
typeof(nrow(data))
folds <- 10
round(nrow(data)/folds)
round(nrow(data)/folds, 0)
folds <- cut(seq(1,nrow(data)),breaks = k)
folds
k_folds <- 10
folds <- cut(seq(1,nrow(data)),breaks = k_folds)
for(i in 1:k){
for (i in 1:nrow(data)){
knn.fit = kknn(R1~.,data[-i,],data[i,], k = , scale = TRUE) # use scaled data
fit_matrix[i,j] <- fitted(knn.fit)
}
}
# SVM
library("kernlab")
# KNN
library("kknn")
for(i in 1:k){
for (i in 1:nrow(data)){
knn.fit = kknn(R1~.,data[-i,],data[i,], k = , scale = TRUE) # use scaled data
fit_matrix[i,j] <- fitted(knn.fit)
}
}
fit_matrix
folds <- cut(seq(1,nrow(data)),breaks = k_folds, labels = FALSE)
folds
test <- data[which(folds == 2,arr.ind = T)]
test <- data[which(folds == 2,arr.ind = T), ]
test
str(test)
?which
test <- data[which(folds == 2), ]
test
str(test)
training <- data[which(folds == -2), ]
str(training)
folds
train <- data[-test,]
test <- data[which(folds == i), ]
str(test)
folds <- cut(seq(1,nrow(data)),breaks = k_folds, labels = FALSE)
folds
test <- data[which(folds == 2), ]
str(test)
train <- data[-test,]
train <- data[which(folds == -2),]
str(train)
index <- which(folds == 2)
str(index)
train <- data[which(folds == -index),]
train <- data[-index,]
str(train)
?kknn
train$R1
#train the model
knn.fit = kknn(R1~., train, test, k = 10, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted
predicted <- ifelse(predicted > 0.5,1,0)
predicted
table(train$R1, predicted)
length(predicted)
nrow(train)
validation <- data[index, ]
index
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 10, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted <- ifelse(predicted > 0.5,1,0)
table(validation$R1, predicted)
for(i in 1:k){
# find index in dataset
index <- which(folds == i)
train <- data[-index,]
validation <- data[index, ]
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 10, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted <- ifelse(predicted > 0.5,1,0)
table(validation$R1, predicted)
accuracy[k] <- mean(fit_matrix[,k] == data$R1)
}
testIndexes <- which(folds==i,arr.ind=TRUE)
table(validation$R1, predicted)
accuracy <- mean(validation$R1 == predicted)
accuracy
results <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data)),breaks = k_folds, labels = FALSE)
for(i in 1:k){
# find index in dataset
index <- which(folds == i)
train <- data[-index,]
validation <- data[index, ]
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 10, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted <- ifelse(predicted > 0.5,1,0)
table(validation$R1, predicted)
accuracy[i] <- mean(validation$R1 == predicted)
}
accuracy
avg <- sum(accuracy)
avg
avg <- sum(accuracy)/length(accuracy)
avg
data_shuffled <- data[sample(nrow(data)),]
head(data_shuffled)
head(data)
data_shuffled <- data[sample(nrow(data)),]
data_shuffled <- data[sample(nrow(data)),]
# store the results
results <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data_shuffled)),breaks = k_folds, labels = FALSE)
for(i in 1:k){
# find index in dataset
index <- which(folds == i)
train <- data_shuffled[-index,]
validation <- data_shuffled[index, ]
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 10, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted <- ifelse(predicted > 0.5,1,0)
table(validation$R1, predicted)
accuracy[i] <- mean(validation$R1 == predicted)
}
avg <- sum(accuracy)/length(accuracy)
avg
data_shuffled <- data[sample(nrow(data)),]
# store the results
results <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data_shuffled)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data_shuffled[-index,]
validation <- data_shuffled[index, ]
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 10, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted <- ifelse(predicted > 0.5,1,0)
table(validation$R1, predicted)
accuracy[i] <- mean(validation$R1 == predicted)
}
avg <- sum(accuracy)/length(accuracy)
avg
results <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data[-index,]
validation <- data[index, ]
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 10, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted <- ifelse(predicted > 0.5,1,0)
table(validation$R1, predicted)
accuracy[i] <- mean(validation$R1 == predicted)
}
avg <- sum(accuracy)/length(accuracy)
avg
fit_matrix <- matrix(0, nrow = nrow(data), ncol = 10 )
# loop "i" for testing all data -1 data point each time
# loop "j" for testing different values of k each time
for (i in 1:nrow(data)){
for (j in 1:10){
knn.fit = kknn(R1~.,data[-i,],data[i,], k = j, scale = TRUE) # use scaled data
fit_matrix[i,j] <- fitted(knn.fit)
}
}
# transform continuous prediction in binary
fit_matrix <- ifelse(fit_matrix > 0.5,1,0)
colnames(fit_matrix) <- c("K=1", "K=2", "K=3", "K=4", "K=5", "K=6", "K=7", "K=8", "K=9", "K=10")
accuracy <- 0
# accuracy with different values of k <- [1] 0.8149847 0.8149847 0.8149847 0.8149847 0.8516820
# 0.8455657 0.8470948 0.8486239 0.8470948 0.8501529
for (k in 1:10){
table(data[,11], fit_matrix[,k])
accuracy[k] <- mean(fit_matrix[,k] == data$R1)
}
accuracy
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 10, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
length(predicted)
data_shuffled <- data[sample(nrow(data)),]
# store the results
results <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data_shuffled)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data_shuffled[-index,]
validation <- data_shuffled[index, ]
# loop on row of validation for the k-fold
for (j in 1:nrow(validation)){
# loop for testing different values of k neighbor
for (z in 1:10){
knn.fit = kknn(R1~., train, test , k = z, scale = TRUE) # use scaled data
fit_matrix[j,z] <- fitted(knn.fit)
fit_matrix <- ifelse(fit_matrix > 0.5,1,0)
correct <- validation$R1 == fit_matrix
answerx[x, 1] = correct
total = sum(answerx)
}
}
}
k_folds <- 10
folds <- cut(seq(1,nrow(data_shuffled)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data_shuffled[-index,]
validation <- data_shuffled[index, ]
# loop on row of validation for the k-fold
for (j in 1:nrow(validation)){
# loop for testing different values of k neighbor
for (z in 1:10){
knn.fit = kknn(R1~., train, test , k = z, scale = TRUE) # use scaled data
fit_matrix <- fitted(knn.fit)
fit_matrix <- ifelse(fit_matrix > 0.5,1,0)
correct <- validation$R1 == fit_matrix
answerx[x, 1] = correct
total = sum(answerx)
}
}
}
length(fit_matrix)
length(validation)
data_shuffled
validation <- data_shuffled[index = 1,]
folds = 1
folds
k_folds <- 10
folds <- cut(seq(1,nrow(data_shuffled)),breaks = k_folds, labels = FALSE)
folds
index <- which(folds = 2)
total
data_shuffled <- data[sample(nrow(data)),]
# store the results
results <- 0
answerx <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data_shuffled)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data_shuffled[-index,]
validation <- data_shuffled[index, ]
# loop on row of validation for the k-fold
for (j in 1:nrow(validation)){
# loop for testing different values of k neighbor
for (z in 1:10){
knn.fit = kknn(R1~., train, test , k = z, scale = TRUE) # use scaled data
fit_matrix <- fitted(knn.fit)
fit_matrix <- ifelse(fit_matrix > 0.5,1,0)
correct <- validation$R1 == fit_matrix
answerx[x, 1] = correct
total = sum(answerx)
}
}
}
data_shuffled <- data[sample(nrow(data)),]
# store the results
results <- 0
answerx <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data_shuffled)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data_shuffled[-index,]
validation <- data_shuffled[index, ]
# loop on row of validation for the k-fold
for (j in 1:nrow(validation)){
# loop for testing different values of k neighbor
for (z in 1:10){
knn.fit = kknn(R1~., train, test , k = z, scale = TRUE) # use scaled data
fit_matrix <- fitted(knn.fit)
fit_matrix <- ifelse(fit_matrix > 0.5,1,0)
correct <- validation$R1 == fit_matrix
answerx[z, 1] = correct
total = sum(answerx)
}
}
}
data_shuffled <- data[sample(nrow(data)),]
# store the results
results <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data[-index,]
validation <- data[index, ]
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 10, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted <- ifelse(predicted > 0.5,1,0)
table(validation$R1, predicted)
accuracy[i] <- mean(validation$R1 == predicted)
}
avg <- sum(accuracy)/length(accuracy)
avg
results <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data[-index,]
validation <- data[index, ]
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 3, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted <- ifelse(predicted > 0.5,1,0)
table(validation$R1, predicted)
accuracy[i] <- mean(validation$R1 == predicted)
}
avg <- sum(accuracy)/length(accuracy)
avg
results <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data[-index,]
validation <- data[index, ]
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 7, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted <- ifelse(predicted > 0.5,1,0)
table(validation$R1, predicted)
accuracy[i] <- mean(validation$R1 == predicted)
}
avg <- sum(accuracy)/length(accuracy)
avg
# QUESTION 3.1.B
s <- sample(nrow(data), 0.6*nrow(data))
s
# split data
train <- data[s, ]
test <- data[s*-0.2,]
str(test)
str(train)
test <- data[-s*0.2,]
str(test)
str(train)
test <- data[-s,]
str(test)
test <- data[-train,]
s
test <- data[-s,]
# create a sample with 60% of observation
ss <- sample(1:3, size = nrow(data), prob = c(0.7, 0.15, 0.15), replace = TRUE)
ss
# create a sample with 70% of observation
s <- sample(1:3, size = nrow(data), prob = c(0.7, 0.15, 0.15), replace = TRUE)
s
train <- data[s == 1,]
str(train)
str(data)
0.7*654
train <- data[s == 1,]
train <- data[s == 2,]
train <- data[s == 3,]
s <- sample(1:3, size = nrow(data), prob = c(0.7, 0.15, 0.15), replace = TRUE)
train <- data[s == 1,]
validation <- data[s == 2,]
test <- data[s == 3,]
str(validation)
str(test)
svm.fit <- ksvm(train, validation, type="C-svc", kernel = "vanilladot", C = 100, scaled = TRUE)
svm.fit <- ksvm(as.matrix(train[,1:10]), as.factor(validation[,11]), type="C-svc", kernel = "vanilladot", C = 100, scaled = TRUE)
datamatrix <- as.matrix(data)
head(datamatrix)
svm.fit <- ksvm(datamatrix[,1:10], datamatrix[,11], type="C-svc", kernel = "vanilladot", C = 100, scaled = TRUE)
pred <- predict(svm.fit, data[,1:10])
pred
sum(pred == data[,11]) / nrow(data)
?predict
svm.fit <- ksvm(train, validation, type="C-svc", kernel = "vanilladot", C = 100, scaled = TRUE)
svm.fit <- ksvm(as.matrix(train), as.matrix(validation), type="C-svc", kernel = "vanilladot", C = 100, scaled = TRUE)
svm.fit <- ksvm(as.matrix(train[,1:10]), as.matrix(train[,11]), type="C-svc", kernel = "vanilladot", C = 100, scaled = TRUE)
# predict
pred <- predict(svm.fit, validation)
# predict
pred <- predict(svm.fit, validation[,1:10])
sum(pred == data[,11]) / nrow(data)
sum(pred == validation[,11]) / nrow(data)
?HoltWinters
setwd("C:/Users/ROMEST/Downloads/homework/Week 4")
data <- read.table("uscrime.txt", header = T)
set.seed(888)
#### PCA ####
# scale = (x-mean(x))/std(x)
pca.fit <- prcomp(data[,-16], scale = TRUE)
# plot PCA
biplot(pca.fit)
# plot variance reduction for each components. We can see the optimal number of components is 8.
# if we look at the summary(pca.fit), we can see that starting PCA8 the proportion of variance explained is
# less than 1.6%. The cumulative variance explained with 8 components is equal to 94%.
plot(pca.fit, xlab = "Principal components", main = "Variance reduction by principal component")
# create data set with response variable and principal components
data.pca <- cbind(pca.fit$x[,1:8], data$Crime)
data.pca <- as.data.frame(data.pca)
# new data point
p <- cbind(14,0,10,12,15.5,0.640,94,150,1.1,0.120,3.6,3200,20.1,0.04,39)
p <- as.data.frame(p)
colnames(p) <- c("M","So","Ed","Po1","Po2","LF","M.F","Pop","NW","U1","U2","Wealth","Ineq","Prob","Time")
# fit a linear regression model
lm.fit <- lm(V9 ~ ., data = data.pca)
# the linear regression models find the coefficients b0, ..., bL
# intercept
b0 <- lm.fit$coefficients[1]
# others coefficients (PC scores)
betas <- lm.fit$coefficients[2:9]
# https://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-com
# PCA reconstruction = PC scores * Eigenvectors(transposed) + Mean
# find mean and standard deviation of predictors
m_pred <- sapply(data[,1:15], mean)
sd_pred <- sapply(data[,1:15], sd)
# Perhaps the most simple, quick and direct way to mean-center your data is by using the
# function scale(). By default, this function will standardize the data (mean zero, unit variance).
# To indicate that we just want to subtract the mean, we need to turn off the argument scale = FALSE.
# unscale the elements by multypling the values t(ik) with b(k)
unval <- pca.fit$rotation[, 1:8] %*% betas
rescale(unval, mean = m_pred, sd = sd_pred,df = T)
?rescale
install.packages("Scale")
rescale(unval, mean = m_pred, sd = sd_pred,df = T)
install.packages("scales")
library(scales)
rescale(unval, mean = m_pred, sd = sd_pred,df = T)
mpred
m_pred
rescale(unval, mean = 0, sd = 1, df = T)
a0 <- b0 - sum(unval*m_pred/sd_pred)
pred <- a0 + sum(a*p)
pred
