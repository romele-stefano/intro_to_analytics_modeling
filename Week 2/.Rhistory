accuracy <- mean(validation$R1 == predicted)
accuracy
results <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data)),breaks = k_folds, labels = FALSE)
for(i in 1:k){
# find index in dataset
index <- which(folds == i)
train <- data[-index,]
validation <- data[index, ]
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 10, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted <- ifelse(predicted > 0.5,1,0)
table(validation$R1, predicted)
accuracy[i] <- mean(validation$R1 == predicted)
}
accuracy
avg <- sum(accuracy)
avg
avg <- sum(accuracy)/length(accuracy)
avg
data_shuffled <- data[sample(nrow(data)),]
head(data_shuffled)
head(data)
data_shuffled <- data[sample(nrow(data)),]
data_shuffled <- data[sample(nrow(data)),]
# store the results
results <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data_shuffled)),breaks = k_folds, labels = FALSE)
for(i in 1:k){
# find index in dataset
index <- which(folds == i)
train <- data_shuffled[-index,]
validation <- data_shuffled[index, ]
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 10, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted <- ifelse(predicted > 0.5,1,0)
table(validation$R1, predicted)
accuracy[i] <- mean(validation$R1 == predicted)
}
avg <- sum(accuracy)/length(accuracy)
avg
data_shuffled <- data[sample(nrow(data)),]
# store the results
results <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data_shuffled)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data_shuffled[-index,]
validation <- data_shuffled[index, ]
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 10, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted <- ifelse(predicted > 0.5,1,0)
table(validation$R1, predicted)
accuracy[i] <- mean(validation$R1 == predicted)
}
avg <- sum(accuracy)/length(accuracy)
avg
results <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data[-index,]
validation <- data[index, ]
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 10, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted <- ifelse(predicted > 0.5,1,0)
table(validation$R1, predicted)
accuracy[i] <- mean(validation$R1 == predicted)
}
avg <- sum(accuracy)/length(accuracy)
avg
fit_matrix <- matrix(0, nrow = nrow(data), ncol = 10 )
# loop "i" for testing all data -1 data point each time
# loop "j" for testing different values of k each time
for (i in 1:nrow(data)){
for (j in 1:10){
knn.fit = kknn(R1~.,data[-i,],data[i,], k = j, scale = TRUE) # use scaled data
fit_matrix[i,j] <- fitted(knn.fit)
}
}
# transform continuous prediction in binary
fit_matrix <- ifelse(fit_matrix > 0.5,1,0)
colnames(fit_matrix) <- c("K=1", "K=2", "K=3", "K=4", "K=5", "K=6", "K=7", "K=8", "K=9", "K=10")
accuracy <- 0
# accuracy with different values of k <- [1] 0.8149847 0.8149847 0.8149847 0.8149847 0.8516820
# 0.8455657 0.8470948 0.8486239 0.8470948 0.8501529
for (k in 1:10){
table(data[,11], fit_matrix[,k])
accuracy[k] <- mean(fit_matrix[,k] == data$R1)
}
accuracy
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 10, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
length(predicted)
data_shuffled <- data[sample(nrow(data)),]
# store the results
results <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data_shuffled)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data_shuffled[-index,]
validation <- data_shuffled[index, ]
# loop on row of validation for the k-fold
for (j in 1:nrow(validation)){
# loop for testing different values of k neighbor
for (z in 1:10){
knn.fit = kknn(R1~., train, test , k = z, scale = TRUE) # use scaled data
fit_matrix[j,z] <- fitted(knn.fit)
fit_matrix <- ifelse(fit_matrix > 0.5,1,0)
correct <- validation$R1 == fit_matrix
answerx[x, 1] = correct
total = sum(answerx)
}
}
}
k_folds <- 10
folds <- cut(seq(1,nrow(data_shuffled)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data_shuffled[-index,]
validation <- data_shuffled[index, ]
# loop on row of validation for the k-fold
for (j in 1:nrow(validation)){
# loop for testing different values of k neighbor
for (z in 1:10){
knn.fit = kknn(R1~., train, test , k = z, scale = TRUE) # use scaled data
fit_matrix <- fitted(knn.fit)
fit_matrix <- ifelse(fit_matrix > 0.5,1,0)
correct <- validation$R1 == fit_matrix
answerx[x, 1] = correct
total = sum(answerx)
}
}
}
length(fit_matrix)
length(validation)
data_shuffled
validation <- data_shuffled[index = 1,]
folds = 1
folds
k_folds <- 10
folds <- cut(seq(1,nrow(data_shuffled)),breaks = k_folds, labels = FALSE)
folds
index <- which(folds = 2)
total
data_shuffled <- data[sample(nrow(data)),]
# store the results
results <- 0
answerx <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data_shuffled)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data_shuffled[-index,]
validation <- data_shuffled[index, ]
# loop on row of validation for the k-fold
for (j in 1:nrow(validation)){
# loop for testing different values of k neighbor
for (z in 1:10){
knn.fit = kknn(R1~., train, test , k = z, scale = TRUE) # use scaled data
fit_matrix <- fitted(knn.fit)
fit_matrix <- ifelse(fit_matrix > 0.5,1,0)
correct <- validation$R1 == fit_matrix
answerx[x, 1] = correct
total = sum(answerx)
}
}
}
data_shuffled <- data[sample(nrow(data)),]
# store the results
results <- 0
answerx <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data_shuffled)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data_shuffled[-index,]
validation <- data_shuffled[index, ]
# loop on row of validation for the k-fold
for (j in 1:nrow(validation)){
# loop for testing different values of k neighbor
for (z in 1:10){
knn.fit = kknn(R1~., train, test , k = z, scale = TRUE) # use scaled data
fit_matrix <- fitted(knn.fit)
fit_matrix <- ifelse(fit_matrix > 0.5,1,0)
correct <- validation$R1 == fit_matrix
answerx[z, 1] = correct
total = sum(answerx)
}
}
}
data_shuffled <- data[sample(nrow(data)),]
# store the results
results <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data[-index,]
validation <- data[index, ]
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 10, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted <- ifelse(predicted > 0.5,1,0)
table(validation$R1, predicted)
accuracy[i] <- mean(validation$R1 == predicted)
}
avg <- sum(accuracy)/length(accuracy)
avg
results <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data[-index,]
validation <- data[index, ]
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 3, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted <- ifelse(predicted > 0.5,1,0)
table(validation$R1, predicted)
accuracy[i] <- mean(validation$R1 == predicted)
}
avg <- sum(accuracy)/length(accuracy)
avg
results <- 0
# set number of folds
k_folds <- 10
folds <- cut(seq(1,nrow(data)),breaks = k_folds, labels = FALSE)
for(i in 1:k_folds){
# find index in dataset
index <- which(folds == i)
train <- data[-index,]
validation <- data[index, ]
#train the model
knn.fit = kknn(R1~., train, validation[,-11], k = 7, scale = TRUE) # use scaled data
predicted <- fitted(knn.fit)
predicted <- ifelse(predicted > 0.5,1,0)
table(validation$R1, predicted)
accuracy[i] <- mean(validation$R1 == predicted)
}
avg <- sum(accuracy)/length(accuracy)
avg
# QUESTION 3.1.B
s <- sample(nrow(data), 0.6*nrow(data))
s
# split data
train <- data[s, ]
test <- data[s*-0.2,]
str(test)
str(train)
test <- data[-s*0.2,]
str(test)
str(train)
test <- data[-s,]
str(test)
test <- data[-train,]
s
test <- data[-s,]
# create a sample with 60% of observation
ss <- sample(1:3, size = nrow(data), prob = c(0.7, 0.15, 0.15), replace = TRUE)
ss
# create a sample with 70% of observation
s <- sample(1:3, size = nrow(data), prob = c(0.7, 0.15, 0.15), replace = TRUE)
s
train <- data[s == 1,]
str(train)
str(data)
0.7*654
train <- data[s == 1,]
train <- data[s == 2,]
train <- data[s == 3,]
s <- sample(1:3, size = nrow(data), prob = c(0.7, 0.15, 0.15), replace = TRUE)
train <- data[s == 1,]
validation <- data[s == 2,]
test <- data[s == 3,]
str(validation)
str(test)
svm.fit <- ksvm(train, validation, type="C-svc", kernel = "vanilladot", C = 100, scaled = TRUE)
svm.fit <- ksvm(as.matrix(train[,1:10]), as.factor(validation[,11]), type="C-svc", kernel = "vanilladot", C = 100, scaled = TRUE)
datamatrix <- as.matrix(data)
head(datamatrix)
svm.fit <- ksvm(datamatrix[,1:10], datamatrix[,11], type="C-svc", kernel = "vanilladot", C = 100, scaled = TRUE)
pred <- predict(svm.fit, data[,1:10])
pred
sum(pred == data[,11]) / nrow(data)
?predict
svm.fit <- ksvm(train, validation, type="C-svc", kernel = "vanilladot", C = 100, scaled = TRUE)
svm.fit <- ksvm(as.matrix(train), as.matrix(validation), type="C-svc", kernel = "vanilladot", C = 100, scaled = TRUE)
svm.fit <- ksvm(as.matrix(train[,1:10]), as.matrix(train[,11]), type="C-svc", kernel = "vanilladot", C = 100, scaled = TRUE)
# predict
pred <- predict(svm.fit, validation)
# predict
pred <- predict(svm.fit, validation[,1:10])
sum(pred == data[,11]) / nrow(data)
sum(pred == validation[,11]) / nrow(data)
getwd()
setwd("C:/Users/ROMEST/Download")
setwd("C:/Users/ROMEST/Downloads")
library(datasets)
data("iris")
data <- data("iris")
data
summary(iris)
?kmeans
# kmeans clustering
kmean.fit <- kmeans(iris, centers = 3)
summary(iris)
iris
summary(iris)
head(iris)
# kmeans clustering
kmean.fit <- kmeans(iris[,-5], centers = 3)
kmean.fit
getwd()
+sds
s
setwd("C:/Users/ROMEST/Downloads\data 3")
setwd("C:/Users/ROMEST/Downloads\data3")
setwd("C:/Users/ROMEST/Downloads/data3")
data <- read.table("iris.txt")
str(data)
head(data)
# columns are: Sepal.Length Sepal.Width Petal.Length Petal.Width Species
colnames(data) <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")
length(data)
data <- as.data.frame(data)
# columns are: Sepal.Length Sepal.Width Petal.Length Petal.Width Species
colnames(data) <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")
ncol(data)
head(data)
str(data)
data <- read.csv("iris.txt")
str(data)
# columns are: Sepal.Length Sepal.Width Petal.Length Petal.Width Species
colnames(data) <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")
str(data)
kmean.fit <- kmeans(iris[,-5], centers = 3)
kmean.fit$results
# kmeans clustering
kmean.fit <- kmeans(iris[,-5], centers = 3)
kmean.fit$results
kmean.fit$cluster
plot(data[c(1,2)], col = kmean.fit$cluster)
par(mfrow=c(1,2), mar=c(5,4,2,2))
plot(data[c(1,2)], col = kmean.fit$cluster)
plot(data[c(1,2)], col = iris.class)
data$class
plot(data[c(1,2)], col = data[,5])
par(mfrow=c(1,2), mar=c(5,4,2,2))
plot(data[c(1,2)], col = kmean.fit$cluster)
plot(data[c(1,2)], col = data[,5])
par(mfrow=c(1,2), mar=c(5,4,2,2), col = c("black", "green", "blue"))
plot(data[c(1,2)], col = kmean.fit$cluster)
plot(data[c(1,2)], col = data[,5])
# kmeans clustering
kmean.fit <- kmeans(iris[,-5], centers = 2)
par(mfrow=c(1,2), mar=c(5,4,2,2))
plot(data[c(1,2)], col = kmean.fit$cluster)
plot(data[c(1,2)], col = data[,5])
table(kmean.fit$cluster, data[,5])
length(kmean.fit$cluster)
nrow(data[,5])
str(data)
data <- read.csv("iris.txt")
str(data)
kmean.fit$cluster
length(kmean.fit$cluster)
data <- read.csv("iris.txt", header = F)
str(data)
colnames(data) <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width", "Species")
# kmeans clustering
kmean.fit <- kmeans(iris[,-5], centers = 2)
# show cluster
kmean.fit$cluster
par(mfrow=c(1,2), mar=c(5,4,2,2))
# plot the prediction
plot(data[c(1,2)], col = kmean.fit$cluster)
# plot real data
plot(data[c(1,2)], col = data[,5])
table(kmean.fit$cluster, data[,5])
mean(kmean.fit$ cluster == data[,5])
# kmeans clustering
kmean.fit <- kmeans(iris[,-5], centers = 3)
# show cluster
kmean.fit$cluster
par(mfrow=c(1,2), mar=c(5,4,2,2))
# plot the prediction
plot(data[c(1,2)], col = kmean.fit$cluster)
# plot real data
plot(data[c(1,2)], col = data[,5])
table(kmean.fit$cluster, data[,5])
table(kmean.fit$cluster, data[,5])
mean(kmean.fit$cluster = data[,5])
mean(kmean.fit$cluster == data[,5])
print(kmean.fit)
# BASIC ALGORITHM
# kmeans clustering
kmean.fit <- kmeans(iris[,-5], centers = 2)
table(kmean.fit$cluster, data[,5])
kmean.fit$betweenss/kmean.fit$totss*100
# BASIC ALGORITHM
# kmeans clustering
kmean.fit <- kmeans(iris[,-5], centers = 3)
table(kmean.fit$cluster, data[,5])
(50+48+36)/(50+48+2+14+36)
kmean.fit$betweenss/kmean.fit$totss*100
set.seed(888)
# BASIC ALGORITHM
# kmeans clustering
kmean.fit <- kmeans(iris[,-5], centers = 3)
par(mfrow=c(1,2), mar=c(5,4,2,2))
# plot the prediction
plot(data[c(1,2)], col = kmean.fit$cluster)
# plot real data
plot(data[c(1,2)], col = data[,5])
table(kmean.fit$cluster, data[,5])
# BASIC ALGORITHM
# kmeans clustering
kmean.fit <- kmeans(iris[,-5], centers = 3)
par(mfrow=c(1,2), mar=c(5,4,2,2))
# plot the prediction
plot(data[c(1,2)], col = kmean.fit$cluster)
# plot real data
plot(data[c(1,2)], col = data[,5])
table(kmean.fit$cluster, data[,5])
mean(kmean.fit$cluster == data[,5])
confusion <- table(kmean.fit$cluster, data[,5])
sum(confusion)
diag(confusion)
# show cluster
kmean.fit$cluster
head(data
)
sum(-diag(confusion))/sum(confusion)
# Iris Setosa = 1, Iris Versicolor = 2, Iris Virginica = 3
confusion <- table(kmean.fit$cluster, data[,5])
confusion
16/150
sum(diag(confusion))/sum(confusion)
str(confusion)
confusion$1
kmean.fit$cluster
kmean.fit$cluster
data[,5]
str(data)
# show cluster
kmean.fit$cluster
levels(data[,5]) <- c("3", "2", "1")
str(data)
sum(kmean.fit$cluster == data[,5])/nrow(data)
# Iris Setosa = 1, Iris Versicolor = 2, Iris Virginica = 3
confusion <- table(kmean.fit$cluster, data[,5])
confusion
(50+48+36)/150
accuracy <- 0
accuracy <- 0
for (k in 1:5){
# kmeans clustering
kmean.fit <- kmeans(iris[,-5], centers = k)
# show cluster
kmean.fit$cluster
par(mfrow=c(1,2), mar=c(5,4,2,2))
# plot the prediction
plot(data[c(1,2)], col = kmean.fit$cluster)
# plot real data
plot(data[c(1,2)], col = data[,5])
accuracy[k] <- sum(kmean.fit$cluster == data[,5])/nrow(data)
}
accuracy
kmean.fit <- kmeans(iris[,-5], centers = 1)
# show cluster
kmean.fit$cluster
par(mfrow=c(1,2), mar=c(5,4,2,2))
# plot the prediction
plot(data[c(1,2)], col = kmean.fit$cluster)
# plot real data
plot(data[c(1,2)], col = data[,5])
# Iris Setosa = 1, Iris Versicolor = 2, Iris Virginica = 3
confusion <- table(kmean.fit$cluster, data[,5])
# find accuracy
sum(kmean.fit$cluster == data[,5])/nrow(data)
confusion
table(data[,5])
50/150
